{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.使用单线程对文本分类的Naive Bayes模型的超参数组合执行网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "news=fetch_20newsgroups(subset='all')\n",
    "print len(news.data)\n",
    "print news.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%time_` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "# 选取前3000条新闻文本进行分割\n",
    "X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf=Pipeline([('vect',TfidfVectorizer(stop_words='english',analyzer='word')),('svc',SVC())])\n",
    "\n",
    "# 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型\n",
    "parameters={'svc_gamma':np.logspace(-2,1,4),'svc_C':np.logspace(-1,1,3)}\n",
    "\n",
    "# 导入网络搜索模块GridSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定\n",
    "gs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3)\n",
    "\n",
    "# 执行单线程网络搜索\n",
    "%time_=gs.fit(X_train,y_train)\n",
    "gs.best_params_,gs.best_score_\n",
    "\n",
    "print gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：\n",
    "\n",
    "- np.logspace(a,b,c):创建等比数列，a为起始点，b为终点，c为元素个数;np.linspace(a,b,c):创建等差数列\n",
    "- refit=True: 使程序以交叉验证训练集得到的最佳超参数重新对所有可用的训练集合开发集进行，作为最终用于性能评估的最佳模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果分析：\n",
    "- 使用单线程网格搜索技术对朴素贝叶斯模型在文本分类任务中的超参数组合进行调优，共有12组超参数X3折交叉验证=36项独立运行的计算任务。寻找到的最佳超参数组合在测试集上所能达成的最高分类准确性为82.27%。\n",
    "- 缺点：耗时\n",
    "- 优点：一旦获取到好超参数组合，则可以保持一段时间使用，是一劳永逸提高模型性能的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.使用多线程对文本分类的Naive Bayes模型的超参数组合执行并行化的网络搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%time_` not found.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "news=fetch_20newsgroups(subset='all')\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "# 选取前3000条新闻文本进行分割\n",
    "X_train,X_test,y_train,y_test=train_test_split(news.data[:3000],news.target[:3000],test_size=0.25,random_state=33)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 导入pipeline，使用pipeline简化系统搭建流程（简化代码），将文本抽取与分类器模型串联起来\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf=Pipeline([('vect',TfidfVectorizer(stop_words='english',analyzer='word')),('svc',SVC())])\n",
    "\n",
    "# 这里需要试验的2个超参数的个数分别是4、3，svc_gamma的参数共有10^-2,10^-1...，则共有12种超参数组合，12种不同参数下的模型\n",
    "parameters={'svc_gamma':np.logspace(-2,1,4),'svc_C':np.logspace(-1,1,3)}\n",
    "\n",
    "# 导入网络搜索模块GridSearchCV\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# 将12组参数组合、初始化的Pipeline和3折交叉验证的要求全部告诉GridSearchCV，注意refit=True的设定\n",
    "# 初始化配置并行网络搜索，n_jobs=-1代表使用该计算机的全部CPU\n",
    "gs=GridSearchCV(clf,parameters,verbose=2,refit=True,cv=3,n_jobs=-1)\n",
    "\n",
    "# 执行多线程并行网络搜索\n",
    "%time_=gs.fit(X_train,y_train)\n",
    "gs.best_params_,gs.best_score_\n",
    "\n",
    "# 输出最佳模型在测试集上的准确性\n",
    "print gs.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果分析：\n",
    "\n",
    "- 相较于单线程，多线程的计算时间仅为51.8秒，且准确性仍为82.27%\n",
    "- 并行搜索：有效利用多核处理器的计算资源，几乎成倍提升运算速度，节省最佳超参数中护额的搜索时间"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
